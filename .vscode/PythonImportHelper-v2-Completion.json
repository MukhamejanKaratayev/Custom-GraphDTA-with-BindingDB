[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "stats",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "stats",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "stats",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "InMemoryDataset",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "InMemoryDataset",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "InMemoryDataset",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch_geometric.loader",
        "description": "torch_geometric.loader",
        "isExtraImport": true,
        "detail": "torch_geometric.loader",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch_geometric.loader",
        "description": "torch_geometric.loader",
        "isExtraImport": true,
        "detail": "torch_geometric.loader",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch_geometric.loader",
        "description": "torch_geometric.loader",
        "isExtraImport": true,
        "detail": "torch_geometric.loader",
        "documentation": {}
    },
    {
        "label": "data",
        "importPath": "torch_geometric",
        "description": "torch_geometric",
        "isExtraImport": true,
        "detail": "torch_geometric",
        "documentation": {}
    },
    {
        "label": "data",
        "importPath": "torch_geometric",
        "description": "torch_geometric",
        "isExtraImport": true,
        "detail": "torch_geometric",
        "documentation": {}
    },
    {
        "label": "data",
        "importPath": "torch_geometric",
        "description": "torch_geometric",
        "isExtraImport": true,
        "detail": "torch_geometric",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ReLU",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "GATConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_max_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "GCNConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "GATConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "GINConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_add_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_mean_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_max_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "GCNConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_max_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "GINConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_add_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_mean_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_max_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "GATConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_max_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "GCNConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "GATConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "GINConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_add_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_mean_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_max_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "GCNConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_max_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "GINConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_add_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_mean_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "global_max_pool",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "sys,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys.",
        "description": "sys.",
        "detail": "sys.",
        "documentation": {}
    },
    {
        "label": "shuffle",
        "importPath": "random",
        "description": "random",
        "isExtraImport": true,
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "shuffle",
        "importPath": "random",
        "description": "random",
        "isExtraImport": true,
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "shuffle",
        "importPath": "random",
        "description": "random",
        "isExtraImport": true,
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "shuffle",
        "importPath": "random",
        "description": "random",
        "isExtraImport": true,
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "GATNet",
        "importPath": "models.gat",
        "description": "models.gat",
        "isExtraImport": true,
        "detail": "models.gat",
        "documentation": {}
    },
    {
        "label": "GATNet",
        "importPath": "models.gat",
        "description": "models.gat",
        "isExtraImport": true,
        "detail": "models.gat",
        "documentation": {}
    },
    {
        "label": "GATNet",
        "importPath": "models.gat",
        "description": "models.gat",
        "isExtraImport": true,
        "detail": "models.gat",
        "documentation": {}
    },
    {
        "label": "GATNet",
        "importPath": "models.gat",
        "description": "models.gat",
        "isExtraImport": true,
        "detail": "models.gat",
        "documentation": {}
    },
    {
        "label": "GAT_GCN",
        "importPath": "models.gat_gcn",
        "description": "models.gat_gcn",
        "isExtraImport": true,
        "detail": "models.gat_gcn",
        "documentation": {}
    },
    {
        "label": "GAT_GCN",
        "importPath": "models.gat_gcn",
        "description": "models.gat_gcn",
        "isExtraImport": true,
        "detail": "models.gat_gcn",
        "documentation": {}
    },
    {
        "label": "GAT_GCN",
        "importPath": "models.gat_gcn",
        "description": "models.gat_gcn",
        "isExtraImport": true,
        "detail": "models.gat_gcn",
        "documentation": {}
    },
    {
        "label": "GAT_GCN",
        "importPath": "models.gat_gcn",
        "description": "models.gat_gcn",
        "isExtraImport": true,
        "detail": "models.gat_gcn",
        "documentation": {}
    },
    {
        "label": "GCNNet",
        "importPath": "models.gcn",
        "description": "models.gcn",
        "isExtraImport": true,
        "detail": "models.gcn",
        "documentation": {}
    },
    {
        "label": "GCNNet",
        "importPath": "models.gcn",
        "description": "models.gcn",
        "isExtraImport": true,
        "detail": "models.gcn",
        "documentation": {}
    },
    {
        "label": "GCNNet",
        "importPath": "models.gcn",
        "description": "models.gcn",
        "isExtraImport": true,
        "detail": "models.gcn",
        "documentation": {}
    },
    {
        "label": "GCNNet",
        "importPath": "models.gcn",
        "description": "models.gcn",
        "isExtraImport": true,
        "detail": "models.gcn",
        "documentation": {}
    },
    {
        "label": "GINConvNet",
        "importPath": "models.ginconv",
        "description": "models.ginconv",
        "isExtraImport": true,
        "detail": "models.ginconv",
        "documentation": {}
    },
    {
        "label": "GINConvNet",
        "importPath": "models.ginconv",
        "description": "models.ginconv",
        "isExtraImport": true,
        "detail": "models.ginconv",
        "documentation": {}
    },
    {
        "label": "GINConvNet",
        "importPath": "models.ginconv",
        "description": "models.ginconv",
        "isExtraImport": true,
        "detail": "models.ginconv",
        "documentation": {}
    },
    {
        "label": "GINConvNet",
        "importPath": "models.ginconv",
        "description": "models.ginconv",
        "isExtraImport": true,
        "detail": "models.ginconv",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "concordance_index",
        "importPath": "lifelines.utils",
        "description": "lifelines.utils",
        "isExtraImport": true,
        "detail": "lifelines.utils",
        "documentation": {}
    },
    {
        "label": "concordance_index",
        "importPath": "lifelines.utils",
        "description": "lifelines.utils",
        "isExtraImport": true,
        "detail": "lifelines.utils",
        "documentation": {}
    },
    {
        "label": "concordance_index",
        "importPath": "lifelines.utils",
        "description": "lifelines.utils",
        "isExtraImport": true,
        "detail": "lifelines.utils",
        "documentation": {}
    },
    {
        "label": "json,pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json.pickle",
        "description": "json.pickle",
        "detail": "json.pickle",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Chem",
        "importPath": "rdkit",
        "description": "rdkit",
        "isExtraImport": true,
        "detail": "rdkit",
        "documentation": {}
    },
    {
        "label": "Chem",
        "importPath": "rdkit",
        "description": "rdkit",
        "isExtraImport": true,
        "detail": "rdkit",
        "documentation": {}
    },
    {
        "label": "Chem",
        "importPath": "rdkit",
        "description": "rdkit",
        "isExtraImport": true,
        "detail": "rdkit",
        "documentation": {}
    },
    {
        "label": "MolFromSmiles",
        "importPath": "rdkit.Chem",
        "description": "rdkit.Chem",
        "isExtraImport": true,
        "detail": "rdkit.Chem",
        "documentation": {}
    },
    {
        "label": "MolFromSmiles",
        "importPath": "rdkit.Chem",
        "description": "rdkit.Chem",
        "isExtraImport": true,
        "detail": "rdkit.Chem",
        "documentation": {}
    },
    {
        "label": "MolFromSmiles",
        "importPath": "rdkit.Chem",
        "description": "rdkit.Chem",
        "isExtraImport": true,
        "detail": "rdkit.Chem",
        "documentation": {}
    },
    {
        "label": "networkx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "networkx",
        "description": "networkx",
        "detail": "networkx",
        "documentation": {}
    },
    {
        "label": "TestbedDataset",
        "kind": 6,
        "importPath": "create_data_tdc.utils",
        "description": "create_data_tdc.utils",
        "peekOfCode": "class TestbedDataset(InMemoryDataset):\n    def __init__(self, root='/tmp', dataset='davis', \n                 xd=None, xt=None, y=None, transform=None,\n                 pre_transform=None,smile_graph=None):\n        #root is required for save preprocessed data, default is '/tmp'\n        super(TestbedDataset, self).__init__(root, transform, pre_transform)\n        # benchmark dataset, default = 'davis'\n        self.dataset = dataset\n        if os.path.isfile(self.processed_paths[0]):\n            print('Pre-processed data found: {}, loading ...'.format(self.processed_paths[0]))",
        "detail": "create_data_tdc.utils",
        "documentation": {}
    },
    {
        "label": "rmse",
        "kind": 2,
        "importPath": "create_data_tdc.utils",
        "description": "create_data_tdc.utils",
        "peekOfCode": "def rmse(y,f):\n    rmse = sqrt(((y - f)**2).mean(axis=0))\n    return rmse\ndef mse(y,f):\n    mse = ((y - f)**2).mean(axis=0)\n    return mse\ndef pearson(y,f):\n    rp = np.corrcoef(y, f)[0,1]\n    return rp\ndef spearman(y,f):",
        "detail": "create_data_tdc.utils",
        "documentation": {}
    },
    {
        "label": "mse",
        "kind": 2,
        "importPath": "create_data_tdc.utils",
        "description": "create_data_tdc.utils",
        "peekOfCode": "def mse(y,f):\n    mse = ((y - f)**2).mean(axis=0)\n    return mse\ndef pearson(y,f):\n    rp = np.corrcoef(y, f)[0,1]\n    return rp\ndef spearman(y,f):\n    rs = stats.spearmanr(y, f)[0]\n    return rs\ndef ci(y,f):",
        "detail": "create_data_tdc.utils",
        "documentation": {}
    },
    {
        "label": "pearson",
        "kind": 2,
        "importPath": "create_data_tdc.utils",
        "description": "create_data_tdc.utils",
        "peekOfCode": "def pearson(y,f):\n    rp = np.corrcoef(y, f)[0,1]\n    return rp\ndef spearman(y,f):\n    rs = stats.spearmanr(y, f)[0]\n    return rs\ndef ci(y,f):\n    ind = np.argsort(y)\n    y = y[ind]\n    f = f[ind]",
        "detail": "create_data_tdc.utils",
        "documentation": {}
    },
    {
        "label": "spearman",
        "kind": 2,
        "importPath": "create_data_tdc.utils",
        "description": "create_data_tdc.utils",
        "peekOfCode": "def spearman(y,f):\n    rs = stats.spearmanr(y, f)[0]\n    return rs\ndef ci(y,f):\n    ind = np.argsort(y)\n    y = y[ind]\n    f = f[ind]\n    i = len(y)-1\n    j = i-1\n    z = 0.0",
        "detail": "create_data_tdc.utils",
        "documentation": {}
    },
    {
        "label": "ci",
        "kind": 2,
        "importPath": "create_data_tdc.utils",
        "description": "create_data_tdc.utils",
        "peekOfCode": "def ci(y,f):\n    ind = np.argsort(y)\n    y = y[ind]\n    f = f[ind]\n    i = len(y)-1\n    j = i-1\n    z = 0.0\n    S = 0.0\n    while i > 0:\n        while j >= 0:",
        "detail": "create_data_tdc.utils",
        "documentation": {}
    },
    {
        "label": "GATNet",
        "kind": 6,
        "importPath": "models.gat",
        "description": "models.gat",
        "peekOfCode": "class GATNet(torch.nn.Module):\n    def __init__(self, num_features_xd=78, n_output=1, num_features_xt=25,\n                     n_filters=32, embed_dim=128, output_dim=128, dropout=0.2):\n        super(GATNet, self).__init__()\n        # graph layers\n        self.gcn1 = GATConv(num_features_xd, num_features_xd, heads=10, dropout=dropout)\n        self.gcn2 = GATConv(num_features_xd * 10, output_dim, dropout=dropout)\n        self.fc_g1 = nn.Linear(output_dim, output_dim)\n        # 1D convolution on protein sequence\n        self.embedding_xt = nn.Embedding(num_features_xt + 1, embed_dim)",
        "detail": "models.gat",
        "documentation": {}
    },
    {
        "label": "GAT_GCN",
        "kind": 6,
        "importPath": "models.gat_gcn",
        "description": "models.gat_gcn",
        "peekOfCode": "class GAT_GCN(torch.nn.Module):\n    def __init__(self, n_output=1, num_features_xd=78, num_features_xt=25,\n                 n_filters=32, embed_dim=128, output_dim=128, dropout=0.2):\n        super(GAT_GCN, self).__init__()\n        self.n_output = n_output\n        self.conv1 = GATConv(num_features_xd, num_features_xd, heads=10)\n        self.conv2 = GCNConv(num_features_xd*10, num_features_xd*10)\n        self.fc_g1 = torch.nn.Linear(num_features_xd*10*2, 1500)\n        self.fc_g2 = torch.nn.Linear(1500, output_dim)\n        self.relu = nn.ReLU()",
        "detail": "models.gat_gcn",
        "documentation": {}
    },
    {
        "label": "GCNNet",
        "kind": 6,
        "importPath": "models.gcn",
        "description": "models.gcn",
        "peekOfCode": "class GCNNet(torch.nn.Module):\n    def __init__(self, n_output=1, n_filters=32, embed_dim=128,num_features_xd=78, num_features_xt=25, output_dim=128, dropout=0.2):\n        super(GCNNet, self).__init__()\n        # SMILES graph branch\n        self.n_output = n_output\n        self.conv1 = GCNConv(num_features_xd, num_features_xd)\n        self.conv2 = GCNConv(num_features_xd, num_features_xd*2)\n        self.conv3 = GCNConv(num_features_xd*2, num_features_xd * 4)\n        self.fc_g1 = torch.nn.Linear(num_features_xd*4, 1024)\n        self.fc_g2 = torch.nn.Linear(1024, output_dim)",
        "detail": "models.gcn",
        "documentation": {}
    },
    {
        "label": "GINConvNet",
        "kind": 6,
        "importPath": "models.ginconv",
        "description": "models.ginconv",
        "peekOfCode": "class GINConvNet(torch.nn.Module):\n    def __init__(self, n_output=1,num_features_xd=78, num_features_xt=25,\n                 n_filters=32, embed_dim=128, output_dim=128, dropout=0.2):\n        super(GINConvNet, self).__init__()\n        dim = 32\n        self.dropout = nn.Dropout(dropout)\n        self.relu = nn.ReLU()\n        self.n_output = n_output\n        # convolution layers\n        nn1 = Sequential(Linear(num_features_xd, dim), ReLU(), Linear(dim, dim))",
        "detail": "models.ginconv",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "old_files.training",
        "description": "old_files.training",
        "peekOfCode": "def train(model, device, train_loader, optimizer, epoch):\n    print('Training on {} samples...'.format(len(train_loader.dataset)))\n    model.train()\n    for batch_idx, data in enumerate(train_loader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        #if batch_idx % LOG_INTERVAL == 0:\n         #   print('Output ' + str(output))\n          #  print('Truth ' + str(data.y.view(-1, 1).float()))",
        "detail": "old_files.training",
        "documentation": {}
    },
    {
        "label": "predicting",
        "kind": 2,
        "importPath": "old_files.training",
        "description": "old_files.training",
        "peekOfCode": "def predicting(model, device, loader):\n    model.eval()\n    total_preds = torch.Tensor()\n    total_labels = torch.Tensor()\n    print('Make prediction for {} samples...'.format(len(loader.dataset)))\n    with torch.no_grad():\n        for data in tqdm(loader):\n            data = data.to(device)\n            output = model(data)\n            total_preds = torch.cat((total_preds.to(device), output), 0)",
        "detail": "old_files.training",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 5,
        "importPath": "old_files.training",
        "description": "old_files.training",
        "peekOfCode": "datasets = [['davis','kiba','Ki','Kd','IC50', 'davis2', 'bdtdc_ic50','bdtdc_kd','bdtdc_ki','bindingdb_ic50','bindingdb_ki','bindingdb_kd'][int(sys.argv[1])]] \nmodeling = [GINConvNet, GATNet, GAT_GCN, GCNNet][int(sys.argv[2])]\nmodel_st = modeling.__name__\ncuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\", \"cuda:6\",\"cuda:7\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005",
        "detail": "old_files.training",
        "documentation": {}
    },
    {
        "label": "modeling",
        "kind": 5,
        "importPath": "old_files.training",
        "description": "old_files.training",
        "peekOfCode": "modeling = [GINConvNet, GATNet, GAT_GCN, GCNNet][int(sys.argv[2])]\nmodel_st = modeling.__name__\ncuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\", \"cuda:6\",\"cuda:7\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20",
        "detail": "old_files.training",
        "documentation": {}
    },
    {
        "label": "model_st",
        "kind": 5,
        "importPath": "old_files.training",
        "description": "old_files.training",
        "peekOfCode": "model_st = modeling.__name__\ncuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\", \"cuda:6\",\"cuda:7\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000",
        "detail": "old_files.training",
        "documentation": {}
    },
    {
        "label": "cuda_name",
        "kind": 5,
        "importPath": "old_files.training",
        "description": "old_files.training",
        "peekOfCode": "cuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\", \"cuda:6\",\"cuda:7\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)",
        "detail": "old_files.training",
        "documentation": {}
    },
    {
        "label": "TRAIN_BATCH_SIZE",
        "kind": 5,
        "importPath": "old_files.training",
        "description": "old_files.training",
        "peekOfCode": "TRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )",
        "detail": "old_files.training",
        "documentation": {}
    },
    {
        "label": "TEST_BATCH_SIZE",
        "kind": 5,
        "importPath": "old_files.training",
        "description": "old_files.training",
        "peekOfCode": "TEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    if dataset == 'Ki' or dataset == 'Kd' or dataset == 'IC50':",
        "detail": "old_files.training",
        "documentation": {}
    },
    {
        "label": "LR",
        "kind": 5,
        "importPath": "old_files.training",
        "description": "old_files.training",
        "peekOfCode": "LR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    if dataset == 'Ki' or dataset == 'Kd' or dataset == 'IC50':\n        processed_data_file_train = 'data/bindingdb/processed/bindingDB_' + dataset + '_train.pt'",
        "detail": "old_files.training",
        "documentation": {}
    },
    {
        "label": "LOG_INTERVAL",
        "kind": 5,
        "importPath": "old_files.training",
        "description": "old_files.training",
        "peekOfCode": "LOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    if dataset == 'Ki' or dataset == 'Kd' or dataset == 'IC50':\n        processed_data_file_train = 'data/bindingdb/processed/bindingDB_' + dataset + '_train.pt'\n        processed_data_file_test = 'data/bindingdb/processed/bindingDB_' + dataset + '_test.pt'",
        "detail": "old_files.training",
        "documentation": {}
    },
    {
        "label": "NUM_EPOCHS",
        "kind": 5,
        "importPath": "old_files.training",
        "description": "old_files.training",
        "peekOfCode": "NUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    if dataset == 'Ki' or dataset == 'Kd' or dataset == 'IC50':\n        processed_data_file_train = 'data/bindingdb/processed/bindingDB_' + dataset + '_train.pt'\n        processed_data_file_test = 'data/bindingdb/processed/bindingDB_' + dataset + '_test.pt'\n    else:",
        "detail": "old_files.training",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "old_files.training_validation",
        "description": "old_files.training_validation",
        "peekOfCode": "def train(model, device, train_loader, optimizer, epoch):\n    print('Training on {} samples...'.format(len(train_loader.dataset)))\n    model.train()\n    for batch_idx, data in enumerate(train_loader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = loss_fn(output, data.y.view(-1, 1).float().to(device))\n        loss.backward()\n        optimizer.step()",
        "detail": "old_files.training_validation",
        "documentation": {}
    },
    {
        "label": "predicting",
        "kind": 2,
        "importPath": "old_files.training_validation",
        "description": "old_files.training_validation",
        "peekOfCode": "def predicting(model, device, loader):\n    model.eval()\n    total_preds = torch.Tensor()\n    total_labels = torch.Tensor()\n    print('Make prediction for {} samples...'.format(len(loader.dataset)))\n    with torch.no_grad():\n        for data in loader:\n            data = data.to(device)\n            output = model(data)\n            total_preds = torch.cat((total_preds, output.cpu()), 0)",
        "detail": "old_files.training_validation",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 5,
        "importPath": "old_files.training_validation",
        "description": "old_files.training_validation",
        "peekOfCode": "datasets = [['davis','kiba'][int(sys.argv[1])]] \nmodeling = [GINConvNet, GATNet, GAT_GCN, GCNNet][int(sys.argv[2])]\nmodel_st = modeling.__name__\ncuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005",
        "detail": "old_files.training_validation",
        "documentation": {}
    },
    {
        "label": "modeling",
        "kind": 5,
        "importPath": "old_files.training_validation",
        "description": "old_files.training_validation",
        "peekOfCode": "modeling = [GINConvNet, GATNet, GAT_GCN, GCNNet][int(sys.argv[2])]\nmodel_st = modeling.__name__\ncuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20",
        "detail": "old_files.training_validation",
        "documentation": {}
    },
    {
        "label": "model_st",
        "kind": 5,
        "importPath": "old_files.training_validation",
        "description": "old_files.training_validation",
        "peekOfCode": "model_st = modeling.__name__\ncuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000",
        "detail": "old_files.training_validation",
        "documentation": {}
    },
    {
        "label": "cuda_name",
        "kind": 5,
        "importPath": "old_files.training_validation",
        "description": "old_files.training_validation",
        "peekOfCode": "cuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)",
        "detail": "old_files.training_validation",
        "documentation": {}
    },
    {
        "label": "TRAIN_BATCH_SIZE",
        "kind": 5,
        "importPath": "old_files.training_validation",
        "description": "old_files.training_validation",
        "peekOfCode": "TRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )",
        "detail": "old_files.training_validation",
        "documentation": {}
    },
    {
        "label": "TEST_BATCH_SIZE",
        "kind": 5,
        "importPath": "old_files.training_validation",
        "description": "old_files.training_validation",
        "peekOfCode": "TEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    processed_data_file_train = 'data/processed/' + dataset + '_train.pt'",
        "detail": "old_files.training_validation",
        "documentation": {}
    },
    {
        "label": "LR",
        "kind": 5,
        "importPath": "old_files.training_validation",
        "description": "old_files.training_validation",
        "peekOfCode": "LR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n    processed_data_file_test = 'data/processed/' + dataset + '_test.pt'",
        "detail": "old_files.training_validation",
        "documentation": {}
    },
    {
        "label": "LOG_INTERVAL",
        "kind": 5,
        "importPath": "old_files.training_validation",
        "description": "old_files.training_validation",
        "peekOfCode": "LOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n    processed_data_file_test = 'data/processed/' + dataset + '_test.pt'\n    if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_test))):",
        "detail": "old_files.training_validation",
        "documentation": {}
    },
    {
        "label": "NUM_EPOCHS",
        "kind": 5,
        "importPath": "old_files.training_validation",
        "description": "old_files.training_validation",
        "peekOfCode": "NUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n    processed_data_file_test = 'data/processed/' + dataset + '_test.pt'\n    if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_test))):\n        print('please run create_data.py to prepare data in pytorch format!')",
        "detail": "old_files.training_validation",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "old_files.training_validation_2",
        "description": "old_files.training_validation_2",
        "peekOfCode": "def train(model, device, train_loader, optimizer, epoch):\n    print('Training on {} samples...'.format(len(train_loader.dataset)))\n    model.train()\n    for batch_idx, data in enumerate(train_loader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = loss_fn(output, data.y.view(-1, 1).float().to(device))\n        loss.backward()\n        optimizer.step()",
        "detail": "old_files.training_validation_2",
        "documentation": {}
    },
    {
        "label": "predicting",
        "kind": 2,
        "importPath": "old_files.training_validation_2",
        "description": "old_files.training_validation_2",
        "peekOfCode": "def predicting(model, device, loader):\n    model.eval()\n    total_preds = torch.Tensor()\n    total_labels = torch.Tensor()\n    print('Make prediction for {} samples...'.format(len(loader.dataset)))\n    with torch.no_grad():\n        for data in tqdm(loader):\n            data = data.to(device)\n            output = model(data)\n            total_preds = torch.cat((total_preds.to(device), output), 0)",
        "detail": "old_files.training_validation_2",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 5,
        "importPath": "old_files.training_validation_2",
        "description": "old_files.training_validation_2",
        "peekOfCode": "datasets = [['davis','kiba','Ki','Kd','IC50'][int(sys.argv[1])]] \nmodeling = [GINConvNet, GATNet, GAT_GCN, GCNNet][int(sys.argv[2])]\nmodel_st = modeling.__name__\ncuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\", \"cuda:6\",\"cuda:7\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005",
        "detail": "old_files.training_validation_2",
        "documentation": {}
    },
    {
        "label": "modeling",
        "kind": 5,
        "importPath": "old_files.training_validation_2",
        "description": "old_files.training_validation_2",
        "peekOfCode": "modeling = [GINConvNet, GATNet, GAT_GCN, GCNNet][int(sys.argv[2])]\nmodel_st = modeling.__name__\ncuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\", \"cuda:6\",\"cuda:7\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20",
        "detail": "old_files.training_validation_2",
        "documentation": {}
    },
    {
        "label": "model_st",
        "kind": 5,
        "importPath": "old_files.training_validation_2",
        "description": "old_files.training_validation_2",
        "peekOfCode": "model_st = modeling.__name__\ncuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\", \"cuda:6\",\"cuda:7\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000",
        "detail": "old_files.training_validation_2",
        "documentation": {}
    },
    {
        "label": "cuda_name",
        "kind": 5,
        "importPath": "old_files.training_validation_2",
        "description": "old_files.training_validation_2",
        "peekOfCode": "cuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\", \"cuda:6\",\"cuda:7\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)",
        "detail": "old_files.training_validation_2",
        "documentation": {}
    },
    {
        "label": "TRAIN_BATCH_SIZE",
        "kind": 5,
        "importPath": "old_files.training_validation_2",
        "description": "old_files.training_validation_2",
        "peekOfCode": "TRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )",
        "detail": "old_files.training_validation_2",
        "documentation": {}
    },
    {
        "label": "TEST_BATCH_SIZE",
        "kind": 5,
        "importPath": "old_files.training_validation_2",
        "description": "old_files.training_validation_2",
        "peekOfCode": "TEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    if dataset == 'Ki' or dataset == 'Kd' or dataset == 'IC50':",
        "detail": "old_files.training_validation_2",
        "documentation": {}
    },
    {
        "label": "LR",
        "kind": 5,
        "importPath": "old_files.training_validation_2",
        "description": "old_files.training_validation_2",
        "peekOfCode": "LR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    if dataset == 'Ki' or dataset == 'Kd' or dataset == 'IC50':\n        processed_data_file_train = 'data/bindingdb/processed/bindingDB_' + dataset + '_train.pt'",
        "detail": "old_files.training_validation_2",
        "documentation": {}
    },
    {
        "label": "LOG_INTERVAL",
        "kind": 5,
        "importPath": "old_files.training_validation_2",
        "description": "old_files.training_validation_2",
        "peekOfCode": "LOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    if dataset == 'Ki' or dataset == 'Kd' or dataset == 'IC50':\n        processed_data_file_train = 'data/bindingdb/processed/bindingDB_' + dataset + '_train.pt'\n        processed_data_file_test = 'data/bindingdb/processed/bindingDB_' + dataset + '_test.pt'",
        "detail": "old_files.training_validation_2",
        "documentation": {}
    },
    {
        "label": "NUM_EPOCHS",
        "kind": 5,
        "importPath": "old_files.training_validation_2",
        "description": "old_files.training_validation_2",
        "peekOfCode": "NUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    if dataset == 'Ki' or dataset == 'Kd' or dataset == 'IC50':\n        processed_data_file_train = 'data/bindingdb/processed/bindingDB_' + dataset + '_train.pt'\n        processed_data_file_test = 'data/bindingdb/processed/bindingDB_' + dataset + '_test.pt'\n    else:",
        "detail": "old_files.training_validation_2",
        "documentation": {}
    },
    {
        "label": "GATNet",
        "kind": 6,
        "importPath": "predict_generated.models.gat",
        "description": "predict_generated.models.gat",
        "peekOfCode": "class GATNet(torch.nn.Module):\n    def __init__(self, num_features_xd=78, n_output=1, num_features_xt=25,\n                     n_filters=32, embed_dim=128, output_dim=128, dropout=0.2):\n        super(GATNet, self).__init__()\n        # graph layers\n        self.gcn1 = GATConv(num_features_xd, num_features_xd, heads=10, dropout=dropout)\n        self.gcn2 = GATConv(num_features_xd * 10, output_dim, dropout=dropout)\n        self.fc_g1 = nn.Linear(output_dim, output_dim)\n        # 1D convolution on protein sequence\n        self.embedding_xt = nn.Embedding(num_features_xt + 1, embed_dim)",
        "detail": "predict_generated.models.gat",
        "documentation": {}
    },
    {
        "label": "GAT_GCN",
        "kind": 6,
        "importPath": "predict_generated.models.gat_gcn",
        "description": "predict_generated.models.gat_gcn",
        "peekOfCode": "class GAT_GCN(torch.nn.Module):\n    def __init__(self, n_output=1, num_features_xd=78, num_features_xt=25,\n                 n_filters=32, embed_dim=128, output_dim=128, dropout=0.2):\n        super(GAT_GCN, self).__init__()\n        self.n_output = n_output\n        self.conv1 = GATConv(num_features_xd, num_features_xd, heads=10)\n        self.conv2 = GCNConv(num_features_xd*10, num_features_xd*10)\n        self.fc_g1 = torch.nn.Linear(num_features_xd*10*2, 1500)\n        self.fc_g2 = torch.nn.Linear(1500, output_dim)\n        self.relu = nn.ReLU()",
        "detail": "predict_generated.models.gat_gcn",
        "documentation": {}
    },
    {
        "label": "GCNNet",
        "kind": 6,
        "importPath": "predict_generated.models.gcn",
        "description": "predict_generated.models.gcn",
        "peekOfCode": "class GCNNet(torch.nn.Module):\n    def __init__(self, n_output=1, n_filters=32, embed_dim=128,num_features_xd=78, num_features_xt=25, output_dim=128, dropout=0.2):\n        super(GCNNet, self).__init__()\n        # SMILES graph branch\n        self.n_output = n_output\n        self.conv1 = GCNConv(num_features_xd, num_features_xd)\n        self.conv2 = GCNConv(num_features_xd, num_features_xd*2)\n        self.conv3 = GCNConv(num_features_xd*2, num_features_xd * 4)\n        self.fc_g1 = torch.nn.Linear(num_features_xd*4, 1024)\n        self.fc_g2 = torch.nn.Linear(1024, output_dim)",
        "detail": "predict_generated.models.gcn",
        "documentation": {}
    },
    {
        "label": "GINConvNet",
        "kind": 6,
        "importPath": "predict_generated.models.ginconv",
        "description": "predict_generated.models.ginconv",
        "peekOfCode": "class GINConvNet(torch.nn.Module):\n    def __init__(self, n_output=1,num_features_xd=78, num_features_xt=25,\n                 n_filters=32, embed_dim=128, output_dim=128, dropout=0.2):\n        super(GINConvNet, self).__init__()\n        dim = 32\n        self.dropout = nn.Dropout(dropout)\n        self.relu = nn.ReLU()\n        self.n_output = n_output\n        # convolution layers\n        nn1 = Sequential(Linear(num_features_xd, dim), ReLU(), Linear(dim, dim))",
        "detail": "predict_generated.models.ginconv",
        "documentation": {}
    },
    {
        "label": "TestbedDataset",
        "kind": 6,
        "importPath": "predict_generated.utils",
        "description": "predict_generated.utils",
        "peekOfCode": "class TestbedDataset(InMemoryDataset):\n    def __init__(self, root='/tmp', dataset='davis', \n                 xd=None, xt=None, y=None, transform=None,\n                 pre_transform=None,smile_graph=None):\n        #root is required for save preprocessed data, default is '/tmp'\n        super(TestbedDataset, self).__init__(root, transform, pre_transform)\n        # benchmark dataset, default = 'davis'\n        self.dataset = dataset\n        if os.path.isfile(self.processed_paths[0]):\n            print('Pre-processed data found: {}, loading ...'.format(self.processed_paths[0]))",
        "detail": "predict_generated.utils",
        "documentation": {}
    },
    {
        "label": "rmse",
        "kind": 2,
        "importPath": "predict_generated.utils",
        "description": "predict_generated.utils",
        "peekOfCode": "def rmse(y,f):\n    rmse = sqrt(((y - f)**2).mean(axis=0))\n    return rmse\ndef mse(y,f):\n    mse = ((y - f)**2).mean(axis=0)\n    return mse\ndef pearson(y,f):\n    rp = np.corrcoef(y, f)[0,1]\n    return rp\ndef spearman(y,f):",
        "detail": "predict_generated.utils",
        "documentation": {}
    },
    {
        "label": "mse",
        "kind": 2,
        "importPath": "predict_generated.utils",
        "description": "predict_generated.utils",
        "peekOfCode": "def mse(y,f):\n    mse = ((y - f)**2).mean(axis=0)\n    return mse\ndef pearson(y,f):\n    rp = np.corrcoef(y, f)[0,1]\n    return rp\ndef spearman(y,f):\n    rs = stats.spearmanr(y, f)[0]\n    return rs\ndef ci(y,f):",
        "detail": "predict_generated.utils",
        "documentation": {}
    },
    {
        "label": "pearson",
        "kind": 2,
        "importPath": "predict_generated.utils",
        "description": "predict_generated.utils",
        "peekOfCode": "def pearson(y,f):\n    rp = np.corrcoef(y, f)[0,1]\n    return rp\ndef spearman(y,f):\n    rs = stats.spearmanr(y, f)[0]\n    return rs\ndef ci(y,f):\n    ind = np.argsort(y)\n    y = y[ind]\n    f = f[ind]",
        "detail": "predict_generated.utils",
        "documentation": {}
    },
    {
        "label": "spearman",
        "kind": 2,
        "importPath": "predict_generated.utils",
        "description": "predict_generated.utils",
        "peekOfCode": "def spearman(y,f):\n    rs = stats.spearmanr(y, f)[0]\n    return rs\ndef ci(y,f):\n    ind = np.argsort(y)\n    y = y[ind]\n    f = f[ind]\n    i = len(y)-1\n    j = i-1\n    z = 0.0",
        "detail": "predict_generated.utils",
        "documentation": {}
    },
    {
        "label": "ci",
        "kind": 2,
        "importPath": "predict_generated.utils",
        "description": "predict_generated.utils",
        "peekOfCode": "def ci(y,f):\n    ind = np.argsort(y)\n    y = y[ind]\n    f = f[ind]\n    i = len(y)-1\n    j = i-1\n    z = 0.0\n    S = 0.0\n    while i > 0:\n        while j >= 0:",
        "detail": "predict_generated.utils",
        "documentation": {}
    },
    {
        "label": "atom_features",
        "kind": 2,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "def atom_features(atom):\n    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb','Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr','Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +\n                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n                    [atom.GetIsAromatic()])\ndef one_of_k_encoding(x, allowable_set):\n    if x not in allowable_set:\n        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n    return list(map(lambda s: x == s, allowable_set))",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "one_of_k_encoding",
        "kind": 2,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "def one_of_k_encoding(x, allowable_set):\n    if x not in allowable_set:\n        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n    return list(map(lambda s: x == s, allowable_set))\ndef one_of_k_encoding_unk(x, allowable_set):\n    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n    if x not in allowable_set:\n        x = allowable_set[-1]\n    return list(map(lambda s: x == s, allowable_set))\ndef smile_to_graph(smile):",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "one_of_k_encoding_unk",
        "kind": 2,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "def one_of_k_encoding_unk(x, allowable_set):\n    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n    if x not in allowable_set:\n        x = allowable_set[-1]\n    return list(map(lambda s: x == s, allowable_set))\ndef smile_to_graph(smile):\n    mol = Chem.MolFromSmiles(smile)\n    c_size = mol.GetNumAtoms()\n    features = []\n    for atom in mol.GetAtoms():",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "smile_to_graph",
        "kind": 2,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "def smile_to_graph(smile):\n    mol = Chem.MolFromSmiles(smile)\n    c_size = mol.GetNumAtoms()\n    features = []\n    for atom in mol.GetAtoms():\n        feature = atom_features(atom)\n        features.append( feature / sum(feature) )\n    edges = []\n    for bond in mol.GetBonds():\n        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "seq_cat",
        "kind": 2,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "def seq_cat(prot):\n    x = np.zeros(max_seq_len)\n    for i, ch in enumerate(prot[:max_seq_len]): \n        x[i] = seq_dict[ch]\n    return x  \n# from DeepDTA data\nall_prots = []\ndatasets = ['kiba','davis']\nfor dataset in datasets:\n    print('convert data from DeepDTA for ', dataset)",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "all_prots",
        "kind": 5,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "all_prots = []\ndatasets = ['kiba','davis']\nfor dataset in datasets:\n    print('convert data from DeepDTA for ', dataset)\n    fpath = 'data/' + dataset + '/'\n    train_fold = json.load(open(fpath + \"folds/train_fold_setting1.txt\"))\n    train_fold = [ee for e in train_fold for ee in e ]\n    valid_fold = json.load(open(fpath + \"folds/test_fold_setting1.txt\"))\n    ligands = json.load(open(fpath + \"ligands_can.txt\"), object_pairs_hook=OrderedDict)\n    proteins = json.load(open(fpath + \"proteins.txt\"), object_pairs_hook=OrderedDict)",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 5,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "datasets = ['kiba','davis']\nfor dataset in datasets:\n    print('convert data from DeepDTA for ', dataset)\n    fpath = 'data/' + dataset + '/'\n    train_fold = json.load(open(fpath + \"folds/train_fold_setting1.txt\"))\n    train_fold = [ee for e in train_fold for ee in e ]\n    valid_fold = json.load(open(fpath + \"folds/test_fold_setting1.txt\"))\n    ligands = json.load(open(fpath + \"ligands_can.txt\"), object_pairs_hook=OrderedDict)\n    proteins = json.load(open(fpath + \"proteins.txt\"), object_pairs_hook=OrderedDict)\n    affinity = pickle.load(open(fpath + \"Y\",\"rb\"), encoding='latin1')",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "seq_voc",
        "kind": 5,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "seq_voc = \"ABCDEFGHIKLMNOPQRSTUVWXYZ\"\nseq_dict = {v:(i+1) for i,v in enumerate(seq_voc)}\nseq_dict_len = len(seq_dict)\nmax_seq_len = 1000\ncompound_iso_smiles = []\nfor dt_name in ['kiba','davis']:\n    opts = ['train','test']\n    for opt in opts:\n        df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n        compound_iso_smiles += list( df['compound_iso_smiles'] )",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "seq_dict",
        "kind": 5,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "seq_dict = {v:(i+1) for i,v in enumerate(seq_voc)}\nseq_dict_len = len(seq_dict)\nmax_seq_len = 1000\ncompound_iso_smiles = []\nfor dt_name in ['kiba','davis']:\n    opts = ['train','test']\n    for opt in opts:\n        df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n        compound_iso_smiles += list( df['compound_iso_smiles'] )\ncompound_iso_smiles = set(compound_iso_smiles)",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "seq_dict_len",
        "kind": 5,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "seq_dict_len = len(seq_dict)\nmax_seq_len = 1000\ncompound_iso_smiles = []\nfor dt_name in ['kiba','davis']:\n    opts = ['train','test']\n    for opt in opts:\n        df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n        compound_iso_smiles += list( df['compound_iso_smiles'] )\ncompound_iso_smiles = set(compound_iso_smiles)\nsmile_graph = {}",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "max_seq_len",
        "kind": 5,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "max_seq_len = 1000\ncompound_iso_smiles = []\nfor dt_name in ['kiba','davis']:\n    opts = ['train','test']\n    for opt in opts:\n        df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n        compound_iso_smiles += list( df['compound_iso_smiles'] )\ncompound_iso_smiles = set(compound_iso_smiles)\nsmile_graph = {}\nfor smile in compound_iso_smiles:",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "compound_iso_smiles",
        "kind": 5,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "compound_iso_smiles = []\nfor dt_name in ['kiba','davis']:\n    opts = ['train','test']\n    for opt in opts:\n        df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n        compound_iso_smiles += list( df['compound_iso_smiles'] )\ncompound_iso_smiles = set(compound_iso_smiles)\nsmile_graph = {}\nfor smile in compound_iso_smiles:\n    g = smile_to_graph(smile)",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "compound_iso_smiles",
        "kind": 5,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "compound_iso_smiles = set(compound_iso_smiles)\nsmile_graph = {}\nfor smile in compound_iso_smiles:\n    g = smile_to_graph(smile)\n    smile_graph[smile] = g\ndatasets = ['davis','kiba']\n# convert to PyTorch data format\nfor dataset in datasets:\n    processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n    processed_data_file_test = 'data/processed/' + dataset + '_test.pt'",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "smile_graph",
        "kind": 5,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "smile_graph = {}\nfor smile in compound_iso_smiles:\n    g = smile_to_graph(smile)\n    smile_graph[smile] = g\ndatasets = ['davis','kiba']\n# convert to PyTorch data format\nfor dataset in datasets:\n    processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n    processed_data_file_test = 'data/processed/' + dataset + '_test.pt'\n    if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_test))):",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 5,
        "importPath": "create_data",
        "description": "create_data",
        "peekOfCode": "datasets = ['davis','kiba']\n# convert to PyTorch data format\nfor dataset in datasets:\n    processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n    processed_data_file_test = 'data/processed/' + dataset + '_test.pt'\n    if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_test))):\n        df = pd.read_csv('data/' + dataset + '_train.csv')\n        train_drugs, train_prots,  train_Y = list(df['compound_iso_smiles']),list(df['target_sequence']),list(df['affinity'])\n        XT = [seq_cat(t) for t in train_prots]\n        train_drugs, train_prots,  train_Y = np.asarray(train_drugs), np.asarray(XT), np.asarray(train_Y)",
        "detail": "create_data",
        "documentation": {}
    },
    {
        "label": "atom_features",
        "kind": 2,
        "importPath": "create_data_bd",
        "description": "create_data_bd",
        "peekOfCode": "def atom_features(atom):\n    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb','Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr','Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +\n                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n                    [atom.GetIsAromatic()])\ndef one_of_k_encoding(x, allowable_set):\n    if x not in allowable_set:\n        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n    return list(map(lambda s: x == s, allowable_set))",
        "detail": "create_data_bd",
        "documentation": {}
    },
    {
        "label": "one_of_k_encoding",
        "kind": 2,
        "importPath": "create_data_bd",
        "description": "create_data_bd",
        "peekOfCode": "def one_of_k_encoding(x, allowable_set):\n    if x not in allowable_set:\n        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n    return list(map(lambda s: x == s, allowable_set))\ndef one_of_k_encoding_unk(x, allowable_set):\n    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n    if x not in allowable_set:\n        x = allowable_set[-1]\n    return list(map(lambda s: x == s, allowable_set))\ndef smile_to_graph(smile):",
        "detail": "create_data_bd",
        "documentation": {}
    },
    {
        "label": "one_of_k_encoding_unk",
        "kind": 2,
        "importPath": "create_data_bd",
        "description": "create_data_bd",
        "peekOfCode": "def one_of_k_encoding_unk(x, allowable_set):\n    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n    if x not in allowable_set:\n        x = allowable_set[-1]\n    return list(map(lambda s: x == s, allowable_set))\ndef smile_to_graph(smile):\n    mol = Chem.MolFromSmiles(smile)\n    c_size = mol.GetNumAtoms()\n    features = []\n    for atom in mol.GetAtoms():",
        "detail": "create_data_bd",
        "documentation": {}
    },
    {
        "label": "smile_to_graph",
        "kind": 2,
        "importPath": "create_data_bd",
        "description": "create_data_bd",
        "peekOfCode": "def smile_to_graph(smile):\n    mol = Chem.MolFromSmiles(smile)\n    c_size = mol.GetNumAtoms()\n    features = []\n    for atom in mol.GetAtoms():\n        feature = atom_features(atom)\n        features.append( feature / sum(feature) )\n    edges = []\n    for bond in mol.GetBonds():\n        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])",
        "detail": "create_data_bd",
        "documentation": {}
    },
    {
        "label": "seq_cat",
        "kind": 2,
        "importPath": "create_data_bd",
        "description": "create_data_bd",
        "peekOfCode": "def seq_cat(prot):\n    x = np.zeros(max_seq_len)\n    for i, ch in enumerate(prot[:max_seq_len]): \n        x[i] = seq_dict[ch]\n    return x \ndatasets = ['IC50']\nseq_voc = \"ABCDEFGHIKLMNOPQRSTUVWXYZ\"\nseq_dict = {v:(i+1) for i,v in enumerate(seq_voc)}\nseq_dict_len = len(seq_dict)\nmax_seq_len = 1000",
        "detail": "create_data_bd",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 5,
        "importPath": "create_data_bd",
        "description": "create_data_bd",
        "peekOfCode": "datasets = ['IC50']\nseq_voc = \"ABCDEFGHIKLMNOPQRSTUVWXYZ\"\nseq_dict = {v:(i+1) for i,v in enumerate(seq_voc)}\nseq_dict_len = len(seq_dict)\nmax_seq_len = 1000\ncompound_iso_smiles = []\nfor dt_name in datasets:\n    opts = ['train','test']\n    for opt in opts:\n        print('Start working on bindingDB_' + str(dt_name) +'_'+str(opt))",
        "detail": "create_data_bd",
        "documentation": {}
    },
    {
        "label": "seq_voc",
        "kind": 5,
        "importPath": "create_data_bd",
        "description": "create_data_bd",
        "peekOfCode": "seq_voc = \"ABCDEFGHIKLMNOPQRSTUVWXYZ\"\nseq_dict = {v:(i+1) for i,v in enumerate(seq_voc)}\nseq_dict_len = len(seq_dict)\nmax_seq_len = 1000\ncompound_iso_smiles = []\nfor dt_name in datasets:\n    opts = ['train','test']\n    for opt in opts:\n        print('Start working on bindingDB_' + str(dt_name) +'_'+str(opt))\n        df = pd.read_csv('data/bindingdb/bindingDB_' + dt_name + '_' + opt + '.csv')",
        "detail": "create_data_bd",
        "documentation": {}
    },
    {
        "label": "seq_dict",
        "kind": 5,
        "importPath": "create_data_bd",
        "description": "create_data_bd",
        "peekOfCode": "seq_dict = {v:(i+1) for i,v in enumerate(seq_voc)}\nseq_dict_len = len(seq_dict)\nmax_seq_len = 1000\ncompound_iso_smiles = []\nfor dt_name in datasets:\n    opts = ['train','test']\n    for opt in opts:\n        print('Start working on bindingDB_' + str(dt_name) +'_'+str(opt))\n        df = pd.read_csv('data/bindingdb/bindingDB_' + dt_name + '_' + opt + '.csv')\n        compound_iso_smiles += list( df['Drug'] )",
        "detail": "create_data_bd",
        "documentation": {}
    },
    {
        "label": "seq_dict_len",
        "kind": 5,
        "importPath": "create_data_bd",
        "description": "create_data_bd",
        "peekOfCode": "seq_dict_len = len(seq_dict)\nmax_seq_len = 1000\ncompound_iso_smiles = []\nfor dt_name in datasets:\n    opts = ['train','test']\n    for opt in opts:\n        print('Start working on bindingDB_' + str(dt_name) +'_'+str(opt))\n        df = pd.read_csv('data/bindingdb/bindingDB_' + dt_name + '_' + opt + '.csv')\n        compound_iso_smiles += list( df['Drug'] )\ncompound_iso_smiles = set(compound_iso_smiles)",
        "detail": "create_data_bd",
        "documentation": {}
    },
    {
        "label": "max_seq_len",
        "kind": 5,
        "importPath": "create_data_bd",
        "description": "create_data_bd",
        "peekOfCode": "max_seq_len = 1000\ncompound_iso_smiles = []\nfor dt_name in datasets:\n    opts = ['train','test']\n    for opt in opts:\n        print('Start working on bindingDB_' + str(dt_name) +'_'+str(opt))\n        df = pd.read_csv('data/bindingdb/bindingDB_' + dt_name + '_' + opt + '.csv')\n        compound_iso_smiles += list( df['Drug'] )\ncompound_iso_smiles = set(compound_iso_smiles)\nsmile_graph = {}",
        "detail": "create_data_bd",
        "documentation": {}
    },
    {
        "label": "compound_iso_smiles",
        "kind": 5,
        "importPath": "create_data_bd",
        "description": "create_data_bd",
        "peekOfCode": "compound_iso_smiles = []\nfor dt_name in datasets:\n    opts = ['train','test']\n    for opt in opts:\n        print('Start working on bindingDB_' + str(dt_name) +'_'+str(opt))\n        df = pd.read_csv('data/bindingdb/bindingDB_' + dt_name + '_' + opt + '.csv')\n        compound_iso_smiles += list( df['Drug'] )\ncompound_iso_smiles = set(compound_iso_smiles)\nsmile_graph = {}\nfor smile in tqdm(compound_iso_smiles):",
        "detail": "create_data_bd",
        "documentation": {}
    },
    {
        "label": "compound_iso_smiles",
        "kind": 5,
        "importPath": "create_data_bd",
        "description": "create_data_bd",
        "peekOfCode": "compound_iso_smiles = set(compound_iso_smiles)\nsmile_graph = {}\nfor smile in tqdm(compound_iso_smiles):\n    g = smile_to_graph(smile)\n    smile_graph[smile] = g\n# convert to PyTorch data format\nfor dataset in datasets:\n    processed_data_file_train = 'data/processed/bindingdb_' + dataset + '_train.pt'\n    processed_data_file_test = 'data/processed/bindingdb_' + dataset + '_test.pt'\n    if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_test))):",
        "detail": "create_data_bd",
        "documentation": {}
    },
    {
        "label": "smile_graph",
        "kind": 5,
        "importPath": "create_data_bd",
        "description": "create_data_bd",
        "peekOfCode": "smile_graph = {}\nfor smile in tqdm(compound_iso_smiles):\n    g = smile_to_graph(smile)\n    smile_graph[smile] = g\n# convert to PyTorch data format\nfor dataset in datasets:\n    processed_data_file_train = 'data/processed/bindingdb_' + dataset + '_train.pt'\n    processed_data_file_test = 'data/processed/bindingdb_' + dataset + '_test.pt'\n    if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_test))):\n        df = pd.read_csv('data/bindingdb/bindingDB_' + dataset + '_train.csv')",
        "detail": "create_data_bd",
        "documentation": {}
    },
    {
        "label": "atom_features",
        "kind": 2,
        "importPath": "create_data_test",
        "description": "create_data_test",
        "peekOfCode": "def atom_features(atom):\n    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb','Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr','Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +\n                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n                    [atom.GetIsAromatic()])\ndef one_of_k_encoding(x, allowable_set):\n    if x not in allowable_set:\n        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n    return list(map(lambda s: x == s, allowable_set))",
        "detail": "create_data_test",
        "documentation": {}
    },
    {
        "label": "one_of_k_encoding",
        "kind": 2,
        "importPath": "create_data_test",
        "description": "create_data_test",
        "peekOfCode": "def one_of_k_encoding(x, allowable_set):\n    if x not in allowable_set:\n        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n    return list(map(lambda s: x == s, allowable_set))\ndef one_of_k_encoding_unk(x, allowable_set):\n    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n    if x not in allowable_set:\n        x = allowable_set[-1]\n    return list(map(lambda s: x == s, allowable_set))\ndef smile_to_graph(smile):",
        "detail": "create_data_test",
        "documentation": {}
    },
    {
        "label": "one_of_k_encoding_unk",
        "kind": 2,
        "importPath": "create_data_test",
        "description": "create_data_test",
        "peekOfCode": "def one_of_k_encoding_unk(x, allowable_set):\n    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n    if x not in allowable_set:\n        x = allowable_set[-1]\n    return list(map(lambda s: x == s, allowable_set))\ndef smile_to_graph(smile):\n    mol = Chem.MolFromSmiles(smile)\n    c_size = mol.GetNumAtoms()\n    features = []\n    for atom in mol.GetAtoms():",
        "detail": "create_data_test",
        "documentation": {}
    },
    {
        "label": "smile_to_graph",
        "kind": 2,
        "importPath": "create_data_test",
        "description": "create_data_test",
        "peekOfCode": "def smile_to_graph(smile):\n    mol = Chem.MolFromSmiles(smile)\n    c_size = mol.GetNumAtoms()\n    features = []\n    for atom in mol.GetAtoms():\n        feature = atom_features(atom)\n        features.append( feature / sum(feature) )\n    edges = []\n    for bond in mol.GetBonds():\n        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])",
        "detail": "create_data_test",
        "documentation": {}
    },
    {
        "label": "seq_cat",
        "kind": 2,
        "importPath": "create_data_test",
        "description": "create_data_test",
        "peekOfCode": "def seq_cat(prot):\n    x = np.zeros(max_seq_len)\n    for i, ch in enumerate(prot[:max_seq_len]): \n        x[i] = seq_dict[ch]\n    return x \n# 'bindingdb_ki','bindingdb_ic50'\ndatasets = ['bdtdc_ki']\nseq_voc = \"ABCDEFGHIKLMNOPQRSTUVWXYZ\"\nseq_dict = {v:(i+1) for i,v in enumerate(seq_voc)}\nseq_dict_len = len(seq_dict)",
        "detail": "create_data_test",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 5,
        "importPath": "create_data_test",
        "description": "create_data_test",
        "peekOfCode": "datasets = ['bdtdc_ki']\nseq_voc = \"ABCDEFGHIKLMNOPQRSTUVWXYZ\"\nseq_dict = {v:(i+1) for i,v in enumerate(seq_voc)}\nseq_dict_len = len(seq_dict)\nmax_seq_len = 1000\ncompound_iso_smiles = []\nfor dt_name in datasets:\n    opts = ['train','test']\n    for opt in opts:\n        print('Start working on ' + str(dt_name) +'_'+str(opt))",
        "detail": "create_data_test",
        "documentation": {}
    },
    {
        "label": "seq_voc",
        "kind": 5,
        "importPath": "create_data_test",
        "description": "create_data_test",
        "peekOfCode": "seq_voc = \"ABCDEFGHIKLMNOPQRSTUVWXYZ\"\nseq_dict = {v:(i+1) for i,v in enumerate(seq_voc)}\nseq_dict_len = len(seq_dict)\nmax_seq_len = 1000\ncompound_iso_smiles = []\nfor dt_name in datasets:\n    opts = ['train','test']\n    for opt in opts:\n        print('Start working on ' + str(dt_name) +'_'+str(opt))\n        df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')",
        "detail": "create_data_test",
        "documentation": {}
    },
    {
        "label": "seq_dict",
        "kind": 5,
        "importPath": "create_data_test",
        "description": "create_data_test",
        "peekOfCode": "seq_dict = {v:(i+1) for i,v in enumerate(seq_voc)}\nseq_dict_len = len(seq_dict)\nmax_seq_len = 1000\ncompound_iso_smiles = []\nfor dt_name in datasets:\n    opts = ['train','test']\n    for opt in opts:\n        print('Start working on ' + str(dt_name) +'_'+str(opt))\n        df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n        compound_iso_smiles += list( df['Drug'] )",
        "detail": "create_data_test",
        "documentation": {}
    },
    {
        "label": "seq_dict_len",
        "kind": 5,
        "importPath": "create_data_test",
        "description": "create_data_test",
        "peekOfCode": "seq_dict_len = len(seq_dict)\nmax_seq_len = 1000\ncompound_iso_smiles = []\nfor dt_name in datasets:\n    opts = ['train','test']\n    for opt in opts:\n        print('Start working on ' + str(dt_name) +'_'+str(opt))\n        df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n        compound_iso_smiles += list( df['Drug'] )\ncompound_iso_smiles = set(compound_iso_smiles)",
        "detail": "create_data_test",
        "documentation": {}
    },
    {
        "label": "max_seq_len",
        "kind": 5,
        "importPath": "create_data_test",
        "description": "create_data_test",
        "peekOfCode": "max_seq_len = 1000\ncompound_iso_smiles = []\nfor dt_name in datasets:\n    opts = ['train','test']\n    for opt in opts:\n        print('Start working on ' + str(dt_name) +'_'+str(opt))\n        df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n        compound_iso_smiles += list( df['Drug'] )\ncompound_iso_smiles = set(compound_iso_smiles)\nsmile_graph = {}",
        "detail": "create_data_test",
        "documentation": {}
    },
    {
        "label": "compound_iso_smiles",
        "kind": 5,
        "importPath": "create_data_test",
        "description": "create_data_test",
        "peekOfCode": "compound_iso_smiles = []\nfor dt_name in datasets:\n    opts = ['train','test']\n    for opt in opts:\n        print('Start working on ' + str(dt_name) +'_'+str(opt))\n        df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n        compound_iso_smiles += list( df['Drug'] )\ncompound_iso_smiles = set(compound_iso_smiles)\nsmile_graph = {}\nfor smile in tqdm(compound_iso_smiles):",
        "detail": "create_data_test",
        "documentation": {}
    },
    {
        "label": "compound_iso_smiles",
        "kind": 5,
        "importPath": "create_data_test",
        "description": "create_data_test",
        "peekOfCode": "compound_iso_smiles = set(compound_iso_smiles)\nsmile_graph = {}\nfor smile in tqdm(compound_iso_smiles):\n    g = smile_to_graph(smile)\n    smile_graph[smile] = g\n# convert to PyTorch data format\nfor dataset in datasets:\n    processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n    processed_data_file_test = 'data/processed/' + dataset + '_test.pt'\n    if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_test))):",
        "detail": "create_data_test",
        "documentation": {}
    },
    {
        "label": "smile_graph",
        "kind": 5,
        "importPath": "create_data_test",
        "description": "create_data_test",
        "peekOfCode": "smile_graph = {}\nfor smile in tqdm(compound_iso_smiles):\n    g = smile_to_graph(smile)\n    smile_graph[smile] = g\n# convert to PyTorch data format\nfor dataset in datasets:\n    processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n    processed_data_file_test = 'data/processed/' + dataset + '_test.pt'\n    if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_test))):\n        df = pd.read_csv('data/' + dataset + '_train.csv')",
        "detail": "create_data_test",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "def train(model, device, train_loader, optimizer, epoch):\n    print('Training on {} samples...'.format(len(train_loader.dataset)))\n    model.train()\n    for batch_idx, data in enumerate(train_loader):\n        data = data.to(device)\n        data.y = data.y.to(device)\n        data.x = data.x.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        #if batch_idx % LOG_INTERVAL == 0:",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "predicting",
        "kind": 2,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "def predicting(model, device, loader):\n    model.eval()\n    total_preds = torch.Tensor()\n    total_labels = torch.Tensor()\n    print('Make prediction for {} samples...'.format(len(loader.dataset)))\n    with torch.no_grad():\n        for data in tqdm(loader):\n            data = data.to(device)\n            output = model(data)\n            total_preds = torch.cat((total_preds.to(device), output), 0)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "datasets = [['davis','kiba','Ki','Kd','IC50', 'davis2', 'bdtdc_ic50','bdtdc_kd','bdtdc_ki','bindingdb_ic50','bindingdb_ki','bindingdb_kd'][int(sys.argv[1])]] \nmodeling = [GINConvNet, GATNet, GAT_GCN, GCNNet][int(sys.argv[2])]\nmodel_st = modeling.__name__\ncuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\", \"cuda:6\",\"cuda:7\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "modeling",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "modeling = [GINConvNet, GATNet, GAT_GCN, GCNNet][int(sys.argv[2])]\nmodel_st = modeling.__name__\ncuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\", \"cuda:6\",\"cuda:7\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "model_st",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "model_st = modeling.__name__\ncuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\", \"cuda:6\",\"cuda:7\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "cuda_name",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "cuda_name = \"cuda:0\"\nif len(sys.argv)>3:\n    cuda_name = [\"cuda:0\",\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\", \"cuda:6\",\"cuda:7\"][int(sys.argv[3])]\nprint('cuda_name:', cuda_name)\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "TRAIN_BATCH_SIZE",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "TRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "TEST_BATCH_SIZE",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "TEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    if dataset == 'Ki' or dataset == 'Kd' or dataset == 'IC50':",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "LR",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "LR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    if dataset == 'Ki' or dataset == 'Kd' or dataset == 'IC50':\n        processed_data_file_train = 'data/bindingdb/processed/bindingDB_' + dataset + '_train.pt'",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "LOG_INTERVAL",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "LOG_INTERVAL = 20\nNUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    if dataset == 'Ki' or dataset == 'Kd' or dataset == 'IC50':\n        processed_data_file_train = 'data/bindingdb/processed/bindingDB_' + dataset + '_train.pt'\n        processed_data_file_test = 'data/bindingdb/processed/bindingDB_' + dataset + '_test.pt'",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "NUM_EPOCHS",
        "kind": 5,
        "importPath": "training",
        "description": "training",
        "peekOfCode": "NUM_EPOCHS = 1000\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    if dataset == 'Ki' or dataset == 'Kd' or dataset == 'IC50':\n        processed_data_file_train = 'data/bindingdb/processed/bindingDB_' + dataset + '_train.pt'\n        processed_data_file_test = 'data/bindingdb/processed/bindingDB_' + dataset + '_test.pt'\n    else:",
        "detail": "training",
        "documentation": {}
    },
    {
        "label": "TestbedDataset",
        "kind": 6,
        "importPath": "utils",
        "description": "utils",
        "peekOfCode": "class TestbedDataset(InMemoryDataset):\n    def __init__(self, root='/tmp', dataset='davis', \n                 xd=None, xt=None, y=None, transform=None,\n                 pre_transform=None,smile_graph=None):\n        #root is required for save preprocessed data, default is '/tmp'\n        super(TestbedDataset, self).__init__(root, transform, pre_transform)\n        # benchmark dataset, default = 'davis'\n        self.dataset = dataset\n        if os.path.isfile(self.processed_paths[0]):\n            print('Pre-processed data found: {}, loading ...'.format(self.processed_paths[0]))",
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "rmse",
        "kind": 2,
        "importPath": "utils",
        "description": "utils",
        "peekOfCode": "def rmse(y,f):\n    rmse = sqrt(((y - f)**2).mean(axis=0))\n    return rmse\ndef mse(y,f):\n    mse = ((y - f)**2).mean(axis=0)\n    return mse\ndef pearson(y,f):\n    rp = np.corrcoef(y, f)[0,1]\n    return rp\ndef spearman(y,f):",
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "mse",
        "kind": 2,
        "importPath": "utils",
        "description": "utils",
        "peekOfCode": "def mse(y,f):\n    mse = ((y - f)**2).mean(axis=0)\n    return mse\ndef pearson(y,f):\n    rp = np.corrcoef(y, f)[0,1]\n    return rp\ndef spearman(y,f):\n    rs = stats.spearmanr(y, f)[0]\n    return rs\ndef ci(y,f):",
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "pearson",
        "kind": 2,
        "importPath": "utils",
        "description": "utils",
        "peekOfCode": "def pearson(y,f):\n    rp = np.corrcoef(y, f)[0,1]\n    return rp\ndef spearman(y,f):\n    rs = stats.spearmanr(y, f)[0]\n    return rs\ndef ci(y,f):\n    ind = np.argsort(y)\n    y = y[ind]\n    f = f[ind]",
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "spearman",
        "kind": 2,
        "importPath": "utils",
        "description": "utils",
        "peekOfCode": "def spearman(y,f):\n    rs = stats.spearmanr(y, f)[0]\n    return rs\ndef ci(y,f):\n    ind = np.argsort(y)\n    y = y[ind]\n    f = f[ind]\n    i = len(y)-1\n    j = i-1\n    z = 0.0",
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "ci",
        "kind": 2,
        "importPath": "utils",
        "description": "utils",
        "peekOfCode": "def ci(y,f):\n    ind = np.argsort(y)\n    y = y[ind]\n    f = f[ind]\n    i = len(y)-1\n    j = i-1\n    z = 0.0\n    S = 0.0\n    while i > 0:\n        while j >= 0:",
        "detail": "utils",
        "documentation": {}
    }
]